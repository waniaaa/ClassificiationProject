# ClassificiationProject

Team Members: Laraib Zia, Iman Durrani, Wania Ismail, Saad Peshiman

Introduction:   We sourced our data from a Kaggle Dataset which included variables such as age, work class, education, race relationship status, and other variables. The variable that we looked to analyze was income level of being either above 50K or below 50K. We wanted to answer the questions of which variables had impacts on income being above 50K or below 50K.  

Data prep:  We prepared our data by creating a clean dataframe called income by dropping all rows with at least 1 NA. This left the dataset with 30162 rows. We then dropped columns which had lowest correlations to the target variable (income >50K), and ones we didn't think would have an impact(capital-loss, native-country, relationship) based on the results from the heatmapping and wrangled data. Finally, we used pandas and numpy to wrangle the data and convert categorical variables to dummy variables in order to run the models. 

EDA:  We used .info(), .head(), and .shape() methods to better understand the dataframes. In order to explore the variables, we made a few plots using seaborn. We plotted the counts of education, race, age, and income >50K. We also plotted barplots of education v income and workclass v income. The analysis methods we utilized include Logistic Regression, Decision Tree Classification, XGBoost, and Neural Networks. Some of the packages we used include pandas, numpy, matplotlib, pyplot, seaborn, sklearn, torch, and torchvision.  

Analysis:  For our dataset, we can conclude that the most fitting algorithm is the XGboost model with an accuracy of 0.851. This is followed by logistic regression (0.839), keras (0.838), and decision tree (0.800). XGboost is the most ideal algorithm evaluation technique for larger datasets, which was optimal for our dataset containing over 30k values. Not only is it one of the quickest algorithms, it was also able to give the most accuracy, making it the most optimal evaluation technique for further predictions. However, one of the disadvantages is that it has the ability to contain high variance, from which variation between the test and training datasets can cause consequential discrepancies with estimating model accuracy.
